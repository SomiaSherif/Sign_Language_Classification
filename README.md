# Sign Language Classification

This project is about developing a machine learning model to classify sign language gestures. The goal is to create a system that can accurately recognize and translate sign language into written or spoken language, improving communication and accessibility for people with hearing impairments.

The project involves collecting a dataset of sign language gestures, preprocessing the data, and training a deep learning model to classify the gestures. The model will be trained using convolutional neural networks (CNNs) and will be fine-tuned using transfer learning.

The performance of the model will be evaluated using standard metrics such as accuracy, precision, recall, and F1 score. The final model will be deployed in a web application that can recognize sign language gestures in real-time using a webcam or a video file.

The project implemented using Python, with the TensorFlow and Keras libraries for deep learning. The code will be published on GitHub as an open-source project, along with the dataset and pre-trained models for others to use and build upon.


**Dataset**: https://www.kaggle.com/datasets/ayuraj/asl-dataset
